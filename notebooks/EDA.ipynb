{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - PneumoniaMNIST\n",
    "\n",
    "This notebook provides comprehensive exploratory data analysis of the PneumoniaMNIST dataset for chest X-ray pneumonia classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import medmnist\n",
    "from medmnist import PneumoniaMNIST\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading PneumoniaMNIST dataset...\")\n",
    "\n",
    "train_dataset = PneumoniaMNIST(split='train', download=True)\n",
    "val_dataset = PneumoniaMNIST(split='val', download=True)\n",
    "test_dataset = PneumoniaMNIST(split='test', download=True)\n",
    "\n",
    "# Get data and labels\n",
    "train_images, train_labels = train_dataset.imgs, train_dataset.labels.flatten()\n",
    "val_images, val_labels = val_dataset.imgs, val_dataset.labels.flatten()\n",
    "test_images, test_labels = test_dataset.imgs, test_dataset.labels.flatten()\n",
    "\n",
    "print(f\"Dataset shapes:\")\n",
    "print(f\"Train: {train_images.shape}, Labels: {train_labels.shape}\")\n",
    "print(f\"Validation: {val_images.shape}, Labels: {val_labels.shape}\")\n",
    "print(f\"Test: {test_images.shape}, Labels: {test_labels.shape}\")\n",
    "\n",
    "# Dataset info\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"Total samples: {len(train_images) + len(val_images) + len(test_images)}\")\n",
    "print(f\"Image dimensions: {train_images.shape[1:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Combine all data for overall analysis\n",
    "all_images = np.concatenate([train_images, val_images, test_images], axis=0)\n",
    "all_labels = np.concatenate([train_labels, val_labels, test_labels], axis=0)\n",
    "\n",
    "# Class distribution\n",
    "class_names = ['Normal', 'Pneumonia']\n",
    "\n",
    "def analyze_class_distribution(labels, split_name):\n",
    "    counts = Counter(labels)\n",
    "    total = len(labels)\n",
    "    \n",
    "    print(f\"\\n{split_name} Class Distribution:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        count = counts[i]\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"{class_name}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    return counts\n",
    "\n",
    "# Analyze each split\n",
    "train_counts = analyze_class_distribution(train_labels, \"Training\")\n",
    "val_counts = analyze_class_distribution(val_labels, \"Validation\")\n",
    "test_counts = analyze_class_distribution(test_labels, \"Test\")\n",
    "all_counts = analyze_class_distribution(all_labels, \"Overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "splits = [('Training', train_labels), ('Validation', val_labels), ('Test', test_labels), ('Overall', all_labels)]\n",
    "\n",
    "for idx, (split_name, labels) in enumerate(splits):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    counts = Counter(labels)\n",
    "    values = [counts[i] for i in range(len(class_names))]\n",
    "    \n",
    "    bars = ax.bar(class_names, values, alpha=0.8)\n",
    "    ax.set_title(f'{split_name} Set Class Distribution')\n",
    "    ax.set_ylabel('Number of Samples')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01*max(values),\n",
    "                f'{value}\\n({value/sum(values)*100:.1f}%)',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Sample images from each class\n",
    "def show_sample_images(images, labels, n_samples=8):\n",
    "    fig, axes = plt.subplots(2, n_samples, figsize=(20, 6))\n",
    "    \n",
    "    for class_idx in range(2):\n",
    "        # Get indices for current class\n",
    "        class_indices = np.where(labels == class_idx)[0]\n",
    "        \n",
    "        # Randomly sample images\n",
    "        sample_indices = np.random.choice(class_indices, n_samples, replace=False)\n",
    "        \n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            ax = axes[class_idx, i]\n",
    "            ax.imshow(images[idx], cmap='gray')\n",
    "            ax.set_title(f'{class_names[class_idx]}\\nSample {i+1}')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Images from Each Class', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_sample_images(all_images, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Pixel intensity analysis\n",
    "def analyze_pixel_intensities(images, labels):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Overall histogram\n",
    "    axes[0].hist(images.flatten(), bins=50, alpha=0.7, density=True)\n",
    "    axes[0].set_title('Overall Pixel Intensity Distribution')\n",
    "    axes[0].set_xlabel('Pixel Intensity')\n",
    "    axes[0].set_ylabel('Density')\n",
    "    \n",
    "    # Class-wise histograms\n",
    "    for class_idx in range(2):\n",
    "        class_images = images[labels == class_idx]\n",
    "        axes[1].hist(class_images.flatten(), bins=50, alpha=0.7, \n",
    "                    label=class_names[class_idx], density=True)\n",
    "    \n",
    "    axes[1].set_title('Pixel Intensity by Class')\n",
    "    axes[1].set_xlabel('Pixel Intensity')\n",
    "    axes[1].set_ylabel('Density')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Mean intensity comparison\n",
    "    mean_intensities = []\n",
    "    for class_idx in range(2):\n",
    "        class_images = images[labels == class_idx]\n",
    "        mean_intensity = np.mean(class_images, axis=(1, 2))\n",
    "        mean_intensities.append(mean_intensity)\n",
    "        axes[2].hist(mean_intensity, bins=30, alpha=0.7, \n",
    "                    label=class_names[class_idx], density=True)\n",
    "    \n",
    "    axes[2].set_title('Mean Image Intensity by Class')\n",
    "    axes[2].set_xlabel('Mean Intensity')\n",
    "    axes[2].set_ylabel('Density')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return mean_intensities\n",
    "\n",
    "mean_intensities = analyze_pixel_intensities(all_images, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Statistical analysis of intensities\n",
    "from scipy import stats\n",
    "\n",
    "print(\"Statistical Analysis of Mean Intensities:\")\n",
    "for class_idx in range(2):\n",
    "    intensities = mean_intensities[class_idx]\n",
    "    print(f\"\\n{class_names[class_idx]}:\")\n",
    "    print(f\"  Mean: {np.mean(intensities):.3f}\")\n",
    "    print(f\"  Std: {np.std(intensities):.3f}\")\n",
    "    print(f\"  Min: {np.min(intensities):.3f}\")\n",
    "    print(f\"  Max: {np.max(intensities):.3f}\")\n",
    "    print(f\"  Median: {np.median(intensities):.3f}\")\n",
    "\n",
    "# Statistical test\n",
    "t_stat, p_value = stats.ttest_ind(mean_intensities[0], mean_intensities[1])\n",
    "print(f\"\\nT-test between classes:\")\n",
    "print(f\"T-statistic: {t_stat:.3f}\")\n",
    "print(f\"P-value: {p_value:.6f}\")\n",
    "print(f\"Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Quality and Preprocessing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze image contrast and other quality metrics\n",
    "def analyze_image_quality(images, labels):\n",
    "    metrics = {\n",
    "        'contrast': [],\n",
    "        'brightness': [],\n",
    "        'sharpness': []\n",
    "    }\n",
    "    \n",
    "    for img in images[:1000]:  # Sample for speed\n",
    "        # Contrast (standard deviation)\n",
    "        contrast = np.std(img)\n",
    "        metrics['contrast'].append(contrast)\n",
    "        \n",
    "        # Brightness (mean)\n",
    "        brightness = np.mean(img)\n",
    "        metrics['brightness'].append(brightness)\n",
    "        \n",
    "        # Sharpness (Laplacian variance)\n",
    "        laplacian = cv2.Laplacian(img, cv2.CV_64F)\n",
    "        sharpness = laplacian.var()\n",
    "        metrics['sharpness'].append(sharpness)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Analyzing image quality metrics (sample of 1000 images)...\")\n",
    "quality_metrics = analyze_image_quality(all_images, all_labels)\n",
    "\n",
    "# Visualize quality metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metric_names = ['Contrast', 'Brightness', 'Sharpness']\n",
    "metric_keys = ['contrast', 'brightness', 'sharpness']\n",
    "\n",
    "for i, (name, key) in enumerate(zip(metric_names, metric_keys)):\n",
    "    axes[i].hist(quality_metrics[key], bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_title(f'{name} Distribution')\n",
    "    axes[i].set_xlabel(name)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Demonstrate preprocessing effects\n",
    "def show_preprocessing_effects(image):\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    \n",
    "    # Original\n",
    "    axes[0, 0].imshow(image, cmap='gray')\n",
    "    axes[0, 0].set_title('Original (28x28)')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Resized to 224x224\n",
    "    resized = cv2.resize(image, (224, 224))\n",
    "    axes[0, 1].imshow(resized, cmap='gray')\n",
    "    axes[0, 1].set_title('Resized (224x224)')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # Normalized\n",
    "    normalized = resized.astype(np.float32) / 255.0\n",
    "    axes[0, 2].imshow(normalized, cmap='gray')\n",
    "    axes[0, 2].set_title('Normalized [0,1]')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # 3-channel\n",
    "    three_channel = np.stack([normalized] * 3, axis=-1)\n",
    "    axes[0, 3].imshow(three_channel)\n",
    "    axes[0, 3].set_title('3-Channel RGB')\n",
    "    axes[0, 3].axis('off')\n",
    "    \n",
    "    # Augmentations\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "    \n",
    "    # Generate augmented images\n",
    "    img_batch = three_channel.reshape(1, 224, 224, 3)\n",
    "    aug_iter = datagen.flow(img_batch, batch_size=1)\n",
    "    \n",
    "    aug_titles = ['Rotation', 'Translation', 'Zoom', 'Horizontal Flip']\n",
    "    \n",
    "    for i in range(4):\n",
    "        aug_img = next(aug_iter)[0]\n",
    "        axes[1, i].imshow(aug_img[:,:,0], cmap='gray')\n",
    "        axes[1, i].set_title(f'Augmented: {aug_titles[i]}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Preprocessing and Augmentation Pipeline', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show preprocessing for a sample image\n",
    "sample_idx = np.random.choice(len(all_images))\n",
    "sample_image = all_images[sample_idx]\n",
    "show_preprocessing_effects(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Imbalance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate class weights for handling imbalance\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def calculate_class_weights(labels):\n",
    "    classes = np.unique(labels)\n",
    "    class_weights = compute_class_weight('balanced', classes=classes, y=labels)\n",
    "    class_weight_dict = dict(zip(classes, class_weights))\n",
    "    return class_weight_dict\n",
    "\n",
    "# Calculate for each split\n",
    "train_weights = calculate_class_weights(train_labels)\n",
    "val_weights = calculate_class_weights(val_labels)\n",
    "test_weights = calculate_class_weights(test_labels)\n",
    "all_weights = calculate_class_weights(all_labels)\n",
    "\n",
    "print(\"Class Weights for Balanced Training:\")\n",
    "print(f\"Training set: {train_weights}\")\n",
    "print(f\"Validation set: {val_weights}\")\n",
    "print(f\"Test set: {test_weights}\")\n",
    "print(f\"Overall: {all_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize imbalance ratio\n",
    "def plot_imbalance_analysis():\n",
    "    splits = ['Train', 'Val', 'Test', 'Overall']\n",
    "    labels_list = [train_labels, val_labels, test_labels, all_labels]\n",
    "    \n",
    "    imbalance_ratios = []\n",
    "    minority_percentages = []\n",
    "    \n",
    "    for labels in labels_list:\n",
    "        counts = Counter(labels)\n",
    "        majority_count = max(counts.values())\n",
    "        minority_count = min(counts.values())\n",
    "        \n",
    "        imbalance_ratio = majority_count / minority_count\n",
    "        minority_percentage = (minority_count / len(labels)) * 100\n",
    "        \n",
    "        imbalance_ratios.append(imbalance_ratio)\n",
    "        minority_percentages.append(minority_percentage)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Imbalance ratio\n",
    "    bars1 = axes[0].bar(splits, imbalance_ratios, alpha=0.8, color='skyblue')\n",
    "    axes[0].set_title('Class Imbalance Ratio (Majority/Minority)')\n",
    "    axes[0].set_ylabel('Ratio')\n",
    "    axes[0].axhline(y=1, color='red', linestyle='--', label='Perfect Balance')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, ratio in zip(bars1, imbalance_ratios):\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{ratio:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Minority class percentage\n",
    "    bars2 = axes[1].bar(splits, minority_percentages, alpha=0.8, color='lightcoral')\n",
    "    axes[1].set_title('Minority Class Percentage')\n",
    "    axes[1].set_ylabel('Percentage (%)')\n",
    "    axes[1].axhline(y=50, color='red', linestyle='--', label='Perfect Balance (50%)')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, percentage in zip(bars2, minority_percentages):\n",
    "        height = bar.get_height()\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                    f'{percentage:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_imbalance_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recommendations for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Summary and recommendations\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET ANALYSIS SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"📊 DATASET CHARACTERISTICS:\")\n",
    "print(f\"• Total samples: {len(all_images):,}\")\n",
    "print(f\"• Image size: {all_images.shape[1:]} (grayscale)\")\n",
    "print(f\"• Classes: {len(class_names)} ({', '.join(class_names)})\")\n",
    "\n",
    "# Class distribution\n",
    "normal_count = np.sum(all_labels == 0)\n",
    "pneumonia_count = np.sum(all_labels == 1)\n",
    "imbalance_ratio = max(normal_count, pneumonia_count) / min(normal_count, pneumonia_count)\n",
    "\n",
    "print(\"⚖️ CLASS IMBALANCE:\")\n",
    "print(f\"• Normal: {normal_count:,} ({normal_count/len(all_labels)*100:.1f}%)\")\n",
    "print(f\"• Pneumonia: {pneumonia_count:,} ({pneumonia_count/len(all_labels)*100:.1f}%)\")\n",
    "print(f\"• Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "print(\"🎯 TRAINING RECOMMENDATIONS:\")\n",
    "print(\"1. DATA PREPROCESSING:\")\n",
    "print(\"   • Resize images from 28x28 to 224x224 for transfer learning\")\n",
    "print(\"   • Normalize pixel values to [0, 1] range\")\n",
    "print(\"   • Convert grayscale to 3-channel by stacking\")\n",
    "print(\"   • Apply data augmentation (rotation ≤10°, translation, zoom, horizontal flip)\")\n",
    "\n",
    "print(\"2. CLASS IMBALANCE HANDLING:\")\n",
    "print(f\"   • Use class weights: {all_weights}\")\n",
    "print(\"   • Consider stratified sampling for train/val/test splits\")\n",
    "print(\"   • Focus on sensitivity/recall for pneumonia detection\")\n",
    "\n",
    "print(\"3. MODEL ARCHITECTURE:\")\n",
    "print(\"   • Start with MobileNetV2 for quick baseline\")\n",
    "print(\"   • Upgrade to ResNet50 for better performance\")\n",
    "print(\"   • Use transfer learning with ImageNet weights\")\n",
    "print(\"   • Add dropout (0.3) and global average pooling\")\n",
    "\n",
    "print(\"4. TRAINING STRATEGY:\")\n",
    "print(\"   • Phase 1: Freeze backbone, train head (5-10 epochs)\")\n",
    "print(\"   • Phase 2: Unfreeze top layers, fine-tune (10-15 epochs)\")\n",
    "print(\"   • Use lower learning rate for fine-tuning (1e-4 → 1e-5)\")\n",
    "print(\"   • Early stopping with patience=6, monitor val_auc\")\n",
    "\n",
    "print(\"5. EVALUATION METRICS:\")\n",
    "print(\"   • Primary: AUC-ROC (handles class imbalance well)\")\n",
    "print(\"   • Clinical: Sensitivity (recall) for pneumonia detection\")\n",
    "print(\"   • Additional: Specificity, F1-score, precision\")\n",
    "print(\"   • Use calibration for reliable probability estimates\")\n",
    "\n",
    "print(\"6. EXPLAINABILITY:\")\n",
    "print(\"   • Implement Grad-CAM for visual explanations\")\n",
    "print(\"   • Focus on lung field regions in heatmaps\")\n",
    "print(\"   • Analyze false positives/negatives with Grad-CAM\")\n",
    "\n",
    "print(\"⚠️ LIMITATIONS TO CONSIDER:\")\n",
    "print(\"   • Small dataset size (may limit generalization)\")\n",
    "print(\"   • Low resolution images (28x28 original)\")\n",
    "print(\"   • Class imbalance requires careful handling\")\n",
    "print(\"   • Not suitable for clinical deployment without validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "Based on this analysis, the next steps in the project should be:\n",
    "\n",
    "1. **Data Preparation**: Run `prepare_data.py` to preprocess and split the data\n",
    "2. **Baseline Model**: Train a simple CNN or MobileNetV2 model\n",
    "3. **Advanced Model**: Implement ResNet50 with transfer learning\n",
    "4. **Evaluation**: Comprehensive evaluation with multiple metrics\n",
    "5. **Explainability**: Implement Grad-CAM for model interpretability\n",
    "6. **Demo Application**: Create Gradio/Streamlit app for interactive use\n",
    "\n",
    "The analysis shows that while the dataset is relatively small and imbalanced, it provides a good foundation for building a pneumonia detection system with proper preprocessing and training strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "R",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
